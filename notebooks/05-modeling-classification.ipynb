{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4dc98b",
   "metadata": {},
   "source": [
    "# Fase 4: Modeling - Clasification (Categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4671437d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Celda 1: Importar librerías y preparar datos\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing  import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.tree            import DecisionTreeClassifier\n",
    "from sklearn.ensemble        import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network  import MLPClassifier\n",
    "from sklearn.metrics         import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Cargar datos\n",
    "data_path = '/Users/luissalamanca/Desktop/Duoc/Machine/ML_Proyecto_Semestral/data/03_features/engineered_data.csv'\n",
    "data = pd.read_csv(data_path, sep=';')\n",
    "\n",
    "# Separar columnas concatenadas\n",
    "if len(data.columns) == 1:\n",
    "    column_name = data.columns[0]\n",
    "    if ',' in column_name:\n",
    "        new_columns = column_name.split(',')\n",
    "        data_split = data[column_name].str.split(',', expand=True)\n",
    "        data_split.columns = new_columns\n",
    "        for col in data_split.columns:\n",
    "            data_split[col] = pd.to_numeric(data_split[col], errors='coerce')\n",
    "        data = data_split\n",
    "\n",
    "# Crear variable objetivo multiclase\n",
    "data['EffectivenessLevel'] = pd.cut(\n",
    "    data['EffectivenessScore'].astype(float),\n",
    "    bins=[-0.1, 0.5, 1.5, 5, np.inf],\n",
    "    labels=['Bajo', 'Medio', 'Alto', 'Experto']\n",
    ")\n",
    "\n",
    "print(data['EffectivenessLevel'].value_counts())\n",
    "print('\\nEstadísticas de EffectivenessScore:\\n', data['EffectivenessScore'].astype(float).describe())\n",
    "\n",
    "# Visualizar distribución de las clases\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.countplot(x='EffectivenessLevel', data=data, order=['Bajo', 'Medio', 'Alto', 'Experto'])\n",
    "plt.title('Distribución de clases: EffectivenessLevel')\n",
    "plt.show()\n",
    "\n",
    "# Lista de features y target\n",
    "features = [\n",
    "    'EconomicEfficiency',\n",
    "    'EquipmentAdvantage',\n",
    "    'KillAssistRatio',\n",
    "    'StealthKillsRatio',\n",
    "    'KDA'\n",
    "]\n",
    "X = data[features]\n",
    "y = data['EffectivenessLevel']\n",
    "\n",
    "# Dividir en entrenamiento/prueba (30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Escalamiento de características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# Convertir los arrays escalados en DataFrame para fácil indexación por nombre\n",
    "X_train_scaled_df = pd.DataFrame(\n",
    "    X_train_scaled,\n",
    "    columns=features,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled_df  = pd.DataFrame(\n",
    "    X_test_scaled,\n",
    "    columns=features,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"Formas de los datos:\")\n",
    "print(f\"  X_train:           {X_train.shape}\")\n",
    "print(f\"  X_test:            {X_test.shape}\")\n",
    "print(f\"  X_train_scaled_df: {X_train_scaled_df.shape}\")\n",
    "print(f\"  X_test_scaled_df:  {X_test_scaled_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba19070",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline y parámetros para GridSearch\n",
    "pipe_lr = Pipeline([\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_lr = {\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(pipe_lr, param_grid_lr, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_lr.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "# Predicciones y métricas\n",
    "y_pred_lr = grid_lr.predict(X_test_scaled_df)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid_lr.best_params_)\n",
    "print(classification_report(y_test, y_pred_lr, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Matriz de confusión - Logistic Regression\")\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb271b31",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_rf = {\n",
    "    'clf__n_estimators': [50, 100],\n",
    "    'clf__max_depth': [None, 10, 20]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(pipe_rf, param_grid_rf, cv=3, scoring='f1', n_jobs=-1)\n",
    "grid_rf.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "y_pred_rf = grid_rf.predict(X_test_scaled_df)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid_rf.best_params_)\n",
    "print(classification_report(y_test, y_pred_rf, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Matriz de confusión - Random Forest\")\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe9387",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pipe_svc = Pipeline([\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "param_grid_svc = {\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "grid_svc = GridSearchCV(pipe_svc, param_grid_svc, cv=3, scoring='f1', n_jobs=-1)\n",
    "grid_svc.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "y_pred_svc = grid_svc.predict(X_test_scaled_df)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid_svc.best_params_)\n",
    "print(classification_report(y_test, y_pred_svc, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_svc)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Matriz de confusión - SVC\")\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a699e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pipe_gb = Pipeline([\n",
    "    ('clf', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_gb = {\n",
    "    'clf__n_estimators': [50, 100],\n",
    "    'clf__learning_rate': [0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_gb = GridSearchCV(pipe_gb, param_grid_gb, cv=3, scoring='f1', n_jobs=-1)\n",
    "grid_gb.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "y_pred_gb = grid_gb.predict(X_test_scaled_df)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid_gb.best_params_)\n",
    "print(classification_report(y_test, y_pred_gb, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_gb)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Matriz de confusión - Gradient Boosting\")\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd04dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pipe_mlp = Pipeline([\n",
    "    ('clf', MLPClassifier(max_iter=300, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_mlp = {\n",
    "    'clf__hidden_layer_sizes': [(50,), (100,)],\n",
    "    'clf__activation': ['relu', 'tanh']\n",
    "}\n",
    "\n",
    "grid_mlp = GridSearchCV(pipe_mlp, param_grid_mlp, cv=3, scoring='f1', n_jobs=-1)\n",
    "grid_mlp.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "y_pred_mlp = grid_mlp.predict(X_test_scaled_df)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid_mlp.best_params_)\n",
    "print(classification_report(y_test, y_pred_mlp, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_mlp)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Matriz de confusión - MLPClassifier\")\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd178a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Análisis comparativo de modelos de clasificación disponibles\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Verificar qué modelos están disponibles\n",
    "modelos_disponibles = {}\n",
    "\n",
    "# Verificar Logistic Regression\n",
    "try:\n",
    "    if 'y_pred_lr' in globals() and 'grid_lr' in globals():\n",
    "        modelos_disponibles['Logistic Regression'] = {\n",
    "            'predicciones': y_pred_lr,\n",
    "            'mejor_modelo': grid_lr.best_estimator_,\n",
    "            'mejores_params': grid_lr.best_params_,\n",
    "            'mejor_score': grid_lr.best_score_\n",
    "        }\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Verificar Random Forest\n",
    "try:\n",
    "    if 'y_pred_rf' in globals() and 'grid_rf' in globals():\n",
    "        modelos_disponibles['Random Forest'] = {\n",
    "            'predicciones': y_pred_rf,\n",
    "            'mejor_modelo': grid_rf.best_estimator_,\n",
    "            'mejores_params': grid_rf.best_params_,\n",
    "            'mejor_score': grid_rf.best_score_\n",
    "        }\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Verificar SVC\n",
    "try:\n",
    "    if 'y_pred_svc' in globals() and 'grid_svc' in globals():\n",
    "        modelos_disponibles['SVC'] = {\n",
    "            'predicciones': y_pred_svc,\n",
    "            'mejor_modelo': grid_svc.best_estimator_,\n",
    "            'mejores_params': grid_svc.best_params_,\n",
    "            'mejor_score': grid_svc.best_score_\n",
    "        }\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Verificar Gradient Boosting\n",
    "try:\n",
    "    if 'y_pred_gb' in globals() and 'grid_gb' in globals():\n",
    "        modelos_disponibles['Gradient Boosting'] = {\n",
    "            'predicciones': y_pred_gb,\n",
    "            'mejor_modelo': grid_gb.best_estimator_,\n",
    "            'mejores_params': grid_gb.best_params_,\n",
    "            'mejor_score': grid_gb.best_score_\n",
    "        }\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Verificar MLP Classifier\n",
    "try:\n",
    "    if 'y_pred_mlp' in globals() and 'grid_mlp' in globals():\n",
    "        modelos_disponibles['MLP Classifier'] = {\n",
    "            'predicciones': y_pred_mlp,\n",
    "            'mejor_modelo': grid_mlp.best_estimator_,\n",
    "            'mejores_params': grid_mlp.best_params_,\n",
    "            'mejor_score': grid_mlp.best_score_\n",
    "        }\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "print(f\"Modelos disponibles para análisis: {list(modelos_disponibles.keys())}\")\n",
    "print(f\"Total de modelos: {len(modelos_disponibles)}\")\n",
    "\n",
    "if len(modelos_disponibles) == 0:\n",
    "    print(\"⚠️ No se encontraron modelos entrenados. Asegúrate de ejecutar las celdas de entrenamiento primero.\")\n",
    "else:\n",
    "    # Calcular métricas para cada modelo disponible\n",
    "    metricas_comparacion = []\n",
    "\n",
    "    for nombre_modelo, datos in modelos_disponibles.items():\n",
    "        y_pred = datos['predicciones']\n",
    "        \n",
    "        metricas = {\n",
    "            'Modelo': nombre_modelo,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'F1-Score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'F1-Score CV': datos['mejor_score']  # Score de validación cruzada\n",
    "        }\n",
    "        metricas_comparacion.append(metricas)\n",
    "\n",
    "    # Crear DataFrame con resultados\n",
    "    df_resultados = pd.DataFrame(metricas_comparacion)\n",
    "    df_resultados = df_resultados.round(4)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANÁLISIS COMPARATIVO DE MODELOS DE CLASIFICACIÓN\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    # Mostrar tabla de resultados\n",
    "    print(\"MÉTRICAS DE RENDIMIENTO:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(df_resultados.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Encontrar el mejor modelo por cada métrica\n",
    "    print(\"MEJORES MODELOS POR MÉTRICA:\")\n",
    "    print(\"-\" * 40)\n",
    "    for metrica in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'F1-Score CV']:\n",
    "        mejor_idx = df_resultados[metrica].idxmax()\n",
    "        mejor_modelo = df_resultados.iloc[mejor_idx]['Modelo']\n",
    "        mejor_valor = df_resultados.iloc[mejor_idx][metrica]\n",
    "        print(f\"{metrica:15}: {mejor_modelo:20} ({mejor_valor:.4f})\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Ranking general (promedio de métricas normalizadas)\n",
    "    metricas_numericas = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'F1-Score CV']\n",
    "    df_normalizado = df_resultados.copy()\n",
    "\n",
    "    for metrica in metricas_numericas:\n",
    "        df_normalizado[metrica] = (df_normalizado[metrica] - df_normalizado[metrica].min()) / \\\n",
    "                                  (df_normalizado[metrica].max() - df_normalizado[metrica].min())\n",
    "\n",
    "    df_normalizado['Score_Promedio'] = df_normalizado[metricas_numericas].mean(axis=1)\n",
    "    df_ranking = df_normalizado[['Modelo', 'Score_Promedio']].sort_values('Score_Promedio', ascending=False)\n",
    "\n",
    "    print(\"RANKING GENERAL DE MODELOS:\")\n",
    "    print(\"-\" * 35)\n",
    "    for i, (_, row) in enumerate(df_ranking.iterrows(), 1):\n",
    "        print(f\"{i}. {row['Modelo']:20} (Score: {row['Score_Promedio']:.4f})\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Mostrar mejores parámetros del modelo ganador\n",
    "    mejor_modelo_nombre = df_ranking.iloc[0]['Modelo']\n",
    "    mejores_params = modelos_disponibles[mejor_modelo_nombre]['mejores_params']\n",
    "\n",
    "    print(f\"MEJORES PARÁMETROS DEL MODELO GANADOR ({mejor_modelo_nombre}):\")\n",
    "    print(\"-\" * 60)\n",
    "    for param, valor in mejores_params.items():\n",
    "        print(f\"{param}: {valor}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Visualización comparativa (solo si hay más de un modelo)\n",
    "    if len(modelos_disponibles) > 1:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "        # Gráfico de barras - Métricas principales\n",
    "        metricas_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "        df_plot = df_resultados.set_index('Modelo')[metricas_plot]\n",
    "        df_plot.plot(kind='bar', ax=axes[0,0], width=0.8)\n",
    "        axes[0,0].set_title('Comparación de Métricas por Modelo')\n",
    "        axes[0,0].set_ylabel('Score')\n",
    "        axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Gráfico de F1-Score\n",
    "        theta = range(len(df_resultados))\n",
    "        f1_scores = df_resultados['F1-Score']\n",
    "        axes[0,1].bar(theta, f1_scores, color='skyblue', alpha=0.7)\n",
    "        axes[0,1].set_title('F1-Score por Modelo')\n",
    "        axes[0,1].set_xticks(theta)\n",
    "        axes[0,1].set_xticklabels(df_resultados['Modelo'], rotation=45, ha='right')\n",
    "        axes[0,1].set_ylabel('F1-Score')\n",
    "\n",
    "        # Heatmap de correlación entre métricas\n",
    "        corr_matrix = df_resultados[metricas_numericas].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,0])\n",
    "        axes[1,0].set_title('Correlación entre Métricas')\n",
    "\n",
    "        # Distribución de scores\n",
    "        df_resultados[metricas_plot].boxplot(ax=axes[1,1])\n",
    "        axes[1,1].set_title('Distribución de Métricas')\n",
    "        axes[1,1].set_ylabel('Score')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Gráfico simple para un solo modelo\n",
    "        metricas_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "        valores = [df_resultados.iloc[0][metrica] for metrica in metricas_plot]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(metricas_plot, valores, color='skyblue', alpha=0.7)\n",
    "        plt.title(f'Métricas del Modelo: {df_resultados.iloc[0][\"Modelo\"]}')\n",
    "        plt.ylabel('Score')\n",
    "        plt.ylim(0, 1)\n",
    "        for i, v in enumerate(valores):\n",
    "            plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "        plt.show()\n",
    "\n",
    "    # Conclusiones finales\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CONCLUSIONES FINALES\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if len(modelos_disponibles) > 1:\n",
    "        # Análisis comparativo\n",
    "        mejor_f1 = df_resultados.loc[df_resultados['F1-Score'].idxmax()]\n",
    "        peor_f1 = df_resultados.loc[df_resultados['F1-Score'].idxmin()]\n",
    "\n",
    "        print(f\"\"\"\n",
    "📊 RESUMEN EJECUTIVO:\n",
    "• Mejor modelo general: {mejor_modelo_nombre}\n",
    "• Mejor F1-Score: {mejor_f1['Modelo']} ({mejor_f1['F1-Score']:.4f})\n",
    "• Mayor Accuracy: {df_resultados.loc[df_resultados['Accuracy'].idxmax(), 'Modelo']} ({df_resultados['Accuracy'].max():.4f})\n",
    "\n",
    "🔍 ANÁLISIS DETALLADO:\n",
    "• Diferencia entre mejor y peor F1-Score: {mejor_f1['F1-Score'] - peor_f1['F1-Score']:.4f}\n",
    "• Modelo más estable: {df_resultados.loc[(df_resultados['F1-Score'] - df_resultados['F1-Score CV']).abs().idxmin(), 'Modelo']}\n",
    "• Promedio general F1-Score: {df_resultados['F1-Score'].mean():.4f}\n",
    "\n",
    "💡 RECOMENDACIONES:\n",
    "• Para producción: Usar {mejor_modelo_nombre} por su rendimiento general superior\n",
    "• Considerar tiempo de entrenamiento e inferencia\n",
    "• Validar con datos no vistos antes del despliegue\n",
    "\"\"\")\n",
    "    else:\n",
    "        # Análisis de modelo único\n",
    "        modelo_unico = df_resultados.iloc[0]\n",
    "        print(f\"\"\"\n",
    "📊 ANÁLISIS DEL MODELO: {modelo_unico['Modelo']}\n",
    "\n",
    "🔍 MÉTRICAS DE RENDIMIENTO:\n",
    "• Accuracy: {modelo_unico['Accuracy']:.4f}\n",
    "• Precision: {modelo_unico['Precision']:.4f}\n",
    "• Recall: {modelo_unico['Recall']:.4f}\n",
    "• F1-Score: {modelo_unico['F1-Score']:.4f}\n",
    "• F1-Score CV: {modelo_unico['F1-Score CV']:.4f}\n",
    "\n",
    "💡 EVALUACIÓN:\n",
    "• Diferencia entre test y CV: {abs(modelo_unico['F1-Score'] - modelo_unico['F1-Score CV']):.4f}\n",
    "• Rendimiento general: {'Excelente' if modelo_unico['F1-Score'] > 0.9 else 'Bueno' if modelo_unico['F1-Score'] > 0.8 else 'Aceptable' if modelo_unico['F1-Score'] > 0.7 else 'Necesita mejora'}\n",
    "\"\"\")\n",
    "\n",
    "    print(\"\\n⚠️  CONSIDERACIONES IMPORTANTES:\")\n",
    "    print(\"• Evaluar el modelo con datos completamente nuevos antes de producción\")\n",
    "    print(\"• Considerar la interpretabilidad según el caso de uso\")\n",
    "    print(\"• Monitorear el rendimiento en producción\")\n",
    "    print(\"• Considerar el tiempo de entrenamiento e inferencia\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
